## Example commands: ##
# On dhogal2 and other machines when using Apptainer
# Uses per-rule Apptainer to prevent dependency collisions
# conda activate snakemake
# snakemake --cores 40 --config run_id=`date +%s` --software-deployment-method apptainer
#
# On laptops and other machines when using monolithic Docker image
# docker load -i docker/nvd.tar
# docker run -it --user $(id -u):$(id -g) -v $(pwd)/:/scratch -w /scratch nvd:30575
# snakemake --cores 10 --config run_id=`date +%s`

import os
import tarfile
import shutil
import pandas as pd
from snakemake.shell import shell

# Import utility functions
from scripts.workflow_functions import (
    get_input_files,
    get_input_type,
    get_sra_accession,
    create_directory,
    upload_file
)

import re

# Use samples from configfile: "config.yaml"
configfile: "config/config.yaml"

# Normalize sample names by replacing dashes, spaces, and special characters with underscores
for sample in config['samples']:
    sample['name'] = re.sub(r'[\s\-]', '_', sample['name'])

# Timestamp set when running starting snakemake run
snakemake_run_id = str(config['run_id'])

# From config.yaml globals
experiment = config['global']['experiment']
blast_db_path = config['global']['blast_db_path'] # downlaoded from NCBI
blast_db_name = config['global']['blast_db_name'] # basename of BLAST db
stat_dbss = config['global']['stat_dbss'] # provided by Kenneth Katz, NCBI
stat_index = config['global']['stat_index'] # provided by Kenneth Katz, NCBI
stat_annotation = config['global']['stat_annotation'] # provided by Kenneth Katz, NCBI
final_output_path = f"{config['global']['out_dir']}/{experiment}" # for local tarball of results
human_virus_taxlist = config['global']['human_virus_taxlist'] # human virus family taxa + arteriviridae

# For FDA-ARGOS mapping
reference_archive = config["global"]["reference_archive"]
reference_decompressed_dir = config["global"]["reference_decompressed_dir"]
reference_sequences = config.get("global", {}).get("reference_sequences", [])

if reference_sequences:
    # Remove file extensions from reference names
    reference_list = [re.sub(r'\.fasta(?:\.gz)?$', '', os.path.basename(ref)) for ref in reference_sequences]
else:
    # Cannot list the directory before it exists, so raise an error
    raise ValueError(
        "No reference sequences specified in config.yaml under 'reference_sequences'. "
        "Please specify the list of reference sequences to use."
    )

# Path to results directory
local_output_dir = 'results'
log_dir = 'logs'

rule all:
    input:
        expand([
            f"{{local_output_dir}}/09_extract_human_virus_contigs/{{sample}}.fa",
            f"{{local_output_dir}}/18_extract_unclassified_contigs/{{sample}}.unclassified.fa",
            f"{{local_output_dir}}/19_labkey/{{sample}}.blast.tkn",
            f"{{local_output_dir}}/19_labkey/{{sample}}.fasta.tkn",
            f"{{local_output_dir}}/20_map_reads_to_contigs/{{sample}}.bam",
            f"{{local_output_dir}}/21_create_tarball/{{sample}}.tar.zst",
            f"{{local_output_dir}}/19_labkey/{{sample}}.upload.done",
        ],
        local_output_dir=local_output_dir,
        sample=[s['name'] for s in config['samples']]
    )
rule prepare_input:
    """
    This workflow expects FASTQ sequences
    These can be from paired-end Illumina reads, ONT sequences, or SRA accessions
    If they are SRA accessions, they will be treated as ONT sequences
    """
    input:
        files = lambda wildcards: get_input_files(wildcards, config)
    output:
        f"{local_output_dir}/01_prepare_input/{{sample}}.fq.gz"
    params:
        input_type = lambda wildcards: get_input_type(wildcards, config),
        sra_accession = lambda wildcards: get_sra_accession(wildcards, config) if get_input_type(wildcards, config) == "sra" else None,
        temp_dir = lambda wildcards: f"{wildcards.sample}.temp",
        sample = "{sample}"
    threads: 4
    log:
        f"{log_dir}/01_prepare_input/{{sample}}.log"
    benchmark:
        f"{log_dir}/01_prepare_input/{{sample}}.txt"
    script:
        "scripts/prepare_input.py"

rule extract_potential_human_virus_family_reads:
    """
    Filter to extract reads that are classified as human-infecting virus families by STAT
    There will be false positives that will need to be dealt with later
    However, this step dramatically reduces the number of reads that need to be examined.
    In testing, 88m reads were reduced to 82.5k reads
    This also dramatically reduces memory requirements which is important for laptop-scale analyses
    And saves a lot of time even on larger servers.

    - Use seqkit to convert FASTQ to FASTA
    - Use aligns_to to classify reads
    - Extract first column with read name
    - Use seqkit to extract reads that align to human-infecting virus families
    - Save output as FASTQ files that can be used as SPAdes input
    """
    input:
        reads = f"{local_output_dir}/01_prepare_input/{{sample}}.fq.gz",
        human_virus_taxlist = human_virus_taxlist,
        stat_dbss = stat_dbss
    output:
        f"{local_output_dir}/02_extract_human_virus_reads/{{sample}}.fq.gz"
    log: f"{log_dir}/02_extract_human_virus_reads/{{sample}}.log"
    benchmark: f"{log_dir}/02_extract_human_virus_reads/{{sample}}.txt"
    threads: 4
    shell:
        """
        seqkit fq2fa --threads 1 {input.reads} \
        | aligns_to.3.1.1 \
        -dbss {input.stat_dbss} \
        -num_threads 2 \
        -tax_list {input.human_virus_taxlist} \
        stdin \
        | cut -f1 \
        | seqkit grep -f - \
        {input.reads} \
        -o {output} 2> {log} | tee {log}
        """

rule run_spades:
    """
    Assemble putative human virus family reads.
    Sewage mode gives excellent results with ONT and Illumina reads.
    """
    input:
        reads = f"{local_output_dir}/02_extract_human_virus_reads/{{sample}}.fq.gz"
    output:
        touch(f"{local_output_dir}/03_de_novo_assemble/{{sample}}/assembly/contigs.fasta")
    threads: 4
    log:
        f"{log_dir}/03_de_novo_assemble/{{sample}}_spades.log"
    benchmark:
        f"{log_dir}/03_de_novo_assemble/{{sample}}_spades.txt"
    params:
        outdir = f"{local_output_dir}/03_de_novo_assemble/{{sample}}/assembly",
        spades_cmd = lambda wildcards, input: (
            "spades.py --sewage --only-assembler -s"
            if get_input_type(wildcards, config) in ['ont', 'sra'] else
            "spades.py --sewage --12"
        )
    shell:
        """
        mkdir -p {params.outdir}
        ({params.spades_cmd} {input.reads} -t {threads} -o {params.outdir} || true) 2>&1 | tee {log}
        """

rule move_spades_output:
    """
    Moves SPAdes contigs to reliably accessible location or create an empty file if assembly failed.
    """
    input:
        contigs = f"{local_output_dir}/03_de_novo_assemble/{{sample}}/assembly/contigs.fasta"
    output:
        final_contigs = f"{local_output_dir}/03_de_novo_assemble/{{sample}}.fa"
    params:
        assembly_dir = f"{local_output_dir}/03_de_novo_assemble/{{sample}}"
    log:
        f"{log_dir}/03_de_novo_assemble/{{sample}}_move.log"
    shell:
        """
        if [ -s {input.contigs} ]; then
            mv {input.contigs} {output.final_contigs}
            rm -rf {params.assembly_dir}
        else
            touch {output.final_contigs}
            echo "No contigs were assembled or SPAdes encountered an error. Created an empty file." >> {log}
        fi
        """

rule mask_low_complexity_contigs:
    """
    Use bbmask to N-mask low complexity sequences that interfere with classification.
    """
    input:
        contigs = f"{local_output_dir}/03_de_novo_assemble/{{sample}}.fa"
    output:
        masked_contigs = f"{local_output_dir}/04_mask_low_complexity_contigs/{{sample}}.fa"
    threads: 1
    resources:
        mem_mb = 8192  # 8GB of memory
    log:
        f"{log_dir}/04_mask_low_complexity_contigs/{{sample}}.log"
    benchmark:
        f"{log_dir}/04_mask_low_complexity_contigs/{{sample}}.txt"
    params:
        entropy = 0.9
    shell:
        """
        if [ -s {input.contigs} ]; then
            bbmask.sh -Xmx{resources.mem_mb}m \
                in={input.contigs} \
                out={output.masked_contigs} \
                entropy={params.entropy} \
                2>&1 | tee {log}
        else
            touch {output.masked_contigs}
            echo "Input file is empty. Created an empty output file." >> {log}
        fi
        """

rule exclude_short_contigs:
    """
    Remove N from ends of sequences and require at least 200bp of called bases after trimming.
    This step is necessary because otherwise BLAST can hang.
    """
    input:
        masked_contigs = f"{local_output_dir}/04_mask_low_complexity_contigs/{{sample}}.fa"
    output:
        filtered_contigs = touch(f"{local_output_dir}/05_exclude_short_contigs/{{sample}}.fa")
    threads: 1
    resources:
        mem_mb = 8192  # 8GB of memory
    log:
        f"{log_dir}/05_exclude_short_contigs/{{sample}}.log"
    benchmark:
        f"{log_dir}/05_exclude_short_contigs/{{sample}}.txt"
    params:
        min_consecutive_bases = 200,
        qtrim = "t"
    shell:
        """
        if [ -s {input.masked_contigs} ]; then
            reformat.sh -Xmx{resources.mem_mb}m \
                in={input.masked_contigs} \
                out={output.filtered_contigs} \
                qtrim={params.qtrim} \
                minconsecutivebases={params.min_consecutive_bases} \
                2>&1 | tee {log}
        else
            touch {output.filtered_contigs}
            echo "Input file is empty. Created an empty output file." >> {log}
        fi
        """

rule classify_contigs_first_pass:
    """
    Classify contigs using NCBI STAT tree_index database
    "Coarse" classification that will be refined in the second pass
    Classification helps identify "viral" contigs that could be better assigned to other taxa
    """
    input:
        fasta_file = f"{local_output_dir}/05_exclude_short_contigs/{{sample}}.fa",
        tree_filter = stat_index
    output:
        first_pass_stat_file = f"{local_output_dir}/06_classify_contigs/{{sample}}.firstpass.txt"
    threads: workflow.cores
    log:
        f"{log_dir}/06_classify_contigs/{{sample}}.classify_contigs_first_pass.log"
    benchmark:
        f"{log_dir}/06_classify_contigs/{{sample}}.classify_contigs_first_pass.txt"
    shell:
        "aligns_to.3.1.1 -dbs {input.tree_filter}  "
        "-num_threads {threads} {input.fasta_file} > {output.first_pass_stat_file} 2> {log}"

rule generate_contigs_tax_list:
    """
    Create list of used taxa based on STAT classification of contigs
    """
    input:
        first_pass_stat_file = f"{local_output_dir}/06_classify_contigs/{{sample}}.firstpass.txt"
    output:
        tax_list = f"{local_output_dir}/06_classify_contigs/tax_list/{{sample}}.tax_list"
    log:
        f"{log_dir}/06_classify_contigs/{{sample}}.generate_tax_list.log"
    benchmark:
        f"{log_dir}/06_classify_contigs/{{sample}}.generate_tax_list.txt"
    script:
        "scripts/generate_tax_list.py"

rule classify_contigs_second_pass:
    """
    Create a second pass classification of contigs using the tax_list generated in the first pass
    By using -dbss, only the taxa that are present in the contigs are further analyzed
    """
    input:
        tax_list = f"{local_output_dir}/06_classify_contigs/tax_list/{{sample}}.tax_list",
        fasta_file = f"{local_output_dir}/05_exclude_short_contigs/{{sample}}.fa",
        tree_filter = stat_dbss
    output:
        second_pass_stat_file = f"{local_output_dir}/06_classify_contigs/{{sample}}.secondpass.txt"
    threads: workflow.cores
    log:
        f"{log_dir}/06_classify_contigs/{{sample}}.classify_reads_second_pass.log"
    benchmark:
        f"{log_dir}/06_classify_contigs/{{sample}}.classify_reads_second_pass.txt"
    shell:
        """
        aligns_to.3.1.1 \
            -tax_list {input.tax_list} \
            -dbss {input.tree_filter} \
            -num_threads 1 \
            {input.fasta_file} \
            > {output.second_pass_stat_file} 2> {log}
        """

rule generate_stat_contig_report:
    """
    generate NCBI STAT report that classifies contig hits
    modify default threshold to >0.001 to report hits more sensitively
    """
    input:
        f"{local_output_dir}/06_classify_contigs/{{sample}}.secondpass.txt"
    output:
        f"{local_output_dir}/07_generate_stat_contig_report/{{sample}}.report"
    threads: 1
    log:
        f"{log_dir}/07_generate_stat_contig_report/{{sample}}.log"
    benchmark:
        f"{log_dir}/07_generate_stat_contig_report/{{sample}}.txt"
    params:
        cutoff_percent = 0.001,
        gettax_sqlite_path = config["global"]["gettax_sqlite_path"]
    script:
        "scripts/hits_to_report.py"

rule identify_human_virus_family_contigs:
    """
    Identifies contigs potentially originating from human viruses.
    This rule filters the classified contigs to focus on those matching known human virus families.
    
    Uses same container as hits_to_report.py since it shares dependencies
    """
    input:
        hits = f"{local_output_dir}/06_classify_contigs/{{sample}}.secondpass.txt"
    output:
        filtered = touch(f"{local_output_dir}/08_identify_human_virus_family_contigs/{{sample}}.txt")
    threads: workflow.cores
    log: f"{log_dir}/08_identify_human_virus_family_contigs/{{sample}}.log"
    benchmark: f"{log_dir}/08_identify_human_virus_family_contigs/{{sample}}.txt"
    params:
        taxa = ["Adenoviridae", "Anelloviridae", "Arenaviridae", "Arteriviridae", 
                "Astroviridae", "Bornaviridae", "Peribunyaviridae", "Caliciviridae", 
                "Coronaviridae", "Filoviridae", "Flaviviridae", "Hepadnaviridae", 
                "Hepeviridae", "Orthoherpesviridae", "Orthomyxoviridae", "Papillomaviridae", 
                "Paramyxoviridae", "Parvoviridae", "Picobirnaviridae", "Picornaviridae", 
                "Pneumoviridae", "Polyomaviridae", "Poxviridae", "Sedoreoviridae", 
                "Retroviridae", "Rhabdoviridae", "Togaviridae", "Kolmioviridae"],
        stringency = 0.7,
        include_children = True
    script:
        "scripts/extract_taxa_spots.py"

rule extract_human_virus_contigs:
    """
    use seqkit to extract contigs from original data that have kmers matching human virus families
    """
    input:
        human_virus_family_hits=f"{local_output_dir}/08_identify_human_virus_family_contigs/{{sample}}.txt",
        fastq=f"{local_output_dir}/05_exclude_short_contigs/{{sample}}.fa"
    output:
        touch(f"{local_output_dir}/09_extract_human_virus_contigs/{{sample}}.fa")
    threads: 1
    log: f"{log_dir}/09_extract_human_virus_contigs/{{sample}}.log"
    benchmark: f"{log_dir}/09_extract_human_virus_contigs/{{sample}}.txt"
    shell:
        """
        seqkit grep -f {input.human_virus_family_hits} {input.fastq} -o {output} 2> {log}
        """

## NCBI BLAST classification of contigs
# For putative human virus family contigs, use two-stage BLAST analysis
# First, analyze contigs with megablast to find close matches to existing targets in core-nt
# Then inspect these results, filtering out those that are phages or are non-viral false positives from STAT
# Next, identify contigs that were not classified by megablast.
# Attempt to classify these contigs with blastn, performing similar filtering on the output 
# Merge the megablast and blastn output into a single file per sample. 

rule megablast:
    """
    Perform rapid sequence similarity search using MEGABLAST.

    This rule aligns the assembled contigs against a comprehensive nucleotide database
    to identify similar known sequences. It's crucial for initial, broad_scale
    identification of viral sequences.
    """
    input:
        contigs = f"{local_output_dir}/09_extract_human_virus_contigs/{{sample}}.fa"
    output:
        blast_results = touch(f"{local_output_dir}/10_megablast/{{sample}}.txt")
    params:
        db = blast_db_path,
        outfmt = "6 qseqid qlen sseqid stitle length pident evalue bitscore sscinames staxids",
        max_target_seqs = 5,
        task = "megablast",
        blastdb = os.environ.get("BLASTDB", "resources/") 
    threads: 4
    log:
        f"{log_dir}/10_megablast/{{sample}}.log"
    benchmark:
        f"{log_dir}/10_megablast/{{sample}}.benchmark.txt"
    shell:
        """
        export BLASTDB={params.blastdb}
        if [ -s {input.contigs} ]; then
            blastn -task {params.task} \
                -db {params.db} \
                -query {input.contigs} \
                -num_threads {threads} \
                -outfmt "{params.outfmt}" \
                -max_target_seqs {params.max_target_seqs} \
                -out {output.blast_results} 2> {log}
        else
            touch {output.blast_results}
            echo "Input file is empty. Created an empty output file." >> {log}
        fi
        """

rule annotate_megablast_results:
    """
    Create file with megablast results annotated with megablast task and full taxonomic rank.
    
    Note:
        gettax.sqlite is created by the hits_to_report.py script and is used to extract full rank
    """
    input:
        blast_results=f"{local_output_dir}/10_megablast/{{sample}}.txt"
    output:
        annotated_results=touch(f"{local_output_dir}/11_annotate_megablast_results/{{sample}}.txt")
    params:
        sample="{sample}",
        gettax_sqlite_path=config["global"]["gettax_sqlite_path"],
        task="megablast"
    threads: 1
    resources:
        mem_mb=1000
    log:
        f"{log_dir}/11_annotate_megablast_results/{{sample}}.log"
    benchmark:
        f"{log_dir}/11_annotate_megablast_results/{{sample}}.benchmark.txt"
    script:
        "scripts/annotate_blast_results.py"

rule filter_non_virus_megablast_nodes:
    """
    Remove any contigs that do not have at least one hit corresponding to viruses.
    
    This rule handles cases where the input file is empty and filters out
    groups where none of the entries are from superkingdom: Viruses
    or where the only viral hits are phages.
    """
    input:
        annotated_results=f"{local_output_dir}/11_annotate_megablast_results/{{sample}}.txt"
    output:
        filtered_results=touch(f"{local_output_dir}/12_filter_non_virus_megablast_nodes/{{sample}}.txt")
    log:
        f"{log_dir}/12_filter_non_virus_megablast_nodes/{{sample}}.log"
    benchmark:
        f"{log_dir}/12_filter_non_virus_megablast_nodes/{{sample}}.benchmark.txt"
    script:
        "scripts/filter_non_virus_megablast_nodes.py"

rule remove_megablast_mapped_contigs:
    """
    Create a list of contigs that are not classified by megablast.
    
    This rule generates two outputs:
    1. A list of classified contigs
    2. A pruned FASTA file containing only unclassified contigs
    
    The pruned file is used as input for more sensitive blastn searching.
    """
    input:
        megablast_results=f"{local_output_dir}/10_megablast/{{sample}}.txt",
        contigs_fasta=f"{local_output_dir}/09_extract_human_virus_contigs/{{sample}}.fa"
    output:
        classified_contigs=f"{local_output_dir}/13_remove_megablast_mapped_contigs/{{sample}}.classified.txt",
        pruned_contigs=f"{local_output_dir}/13_remove_megablast_mapped_contigs/{{sample}}.pruned.fa"
    threads: 1
    log:
        f"{log_dir}/13_remove_megablast_mapped_contigs/{{sample}}.log"
    benchmark:
        f"{log_dir}/13_remove_megablast_mapped_contigs/{{sample}}.benchmark.txt"
    script:
        "scripts/remove_megablast_mapped_contigs.py"

rule blastn_classify:
    """
    Perform sequence similarity search using BLASTN.

    This rule aligns the assembled contigs against a comprehensive nucleotide database
    to identify similar known sequences with BLASTN
    """
    input:
        contigs = f"{local_output_dir}/13_remove_megablast_mapped_contigs/{{sample}}.pruned.fa",
    output:
        blast_results = touch(f"{local_output_dir}/14_blastn_classify/{{sample}}.txt")
    params:
        db = blast_db_path,
        outfmt = "6 qseqid qlen sseqid stitle length pident evalue bitscore sscinames staxids",
        max_target_seqs = 5,
        task = "blastn",
        blastdb = os.environ.get("BLASTDB", "resources/") 
    threads: 4
    log:
        f"{log_dir}/14_blastn_classify/{{sample}}.log"
    benchmark:
        f"{log_dir}/14_blastn_classify/{{sample}}.benchmark.txt"
    shell:
        """
        export BLASTDB={params.blastdb}
        if [ -s {input.contigs} ]; then
            blastn -task {params.task} \
                -db {params.db} \
                -query {input.contigs} \
                -num_threads {threads} \
                -outfmt "{params.outfmt}" \
                -max_target_seqs {params.max_target_seqs} \
                -out {output.blast_results} 2> {log}
        else
            touch {output.blast_results}
            echo "Input file is empty. Created an empty output file." >> {log}
        fi
        """

rule annotate_blastn_results:
    """
    Create file with blastn results annotated with blastn task and full taxonomic rank.
    
    Note:
        gettax.sqlite is created by the hits_to_report.py script.
    """
    input:
        blast_results = f"{local_output_dir}/14_blastn_classify/{{sample}}.txt"
    output:
        annotated_results = touch(f"{local_output_dir}/15_annotate_blastn_results/{{sample}}.txt")
    params:
        sample = "{sample}",
        gettax_sqlite_path = config["global"]["gettax_sqlite_path"],
        task = "blastn"
    threads: 1
    log:
        f"{log_dir}/15_annotate_blastn_results/{{sample}}.log"
    benchmark:
        f"{log_dir}/15_annotate_blastn_results/{{sample}}.benchmark.txt"
    script:
        "scripts/annotate_blast_results.py"

rule filter_non_virus_blastn_nodes:
    """
    Remove any contigs that do not have at least one hit corresponding to viruses.
    
    This rule handles cases where the input file is empty and filters out
    groups where none of the entries are from superkingdom: Viruses
    or where the only viral hits are phages.
    """
    input:
        annotated_results = f"{local_output_dir}/15_annotate_blastn_results/{{sample}}.txt"
    output:
        filtered_results = touch(f"{local_output_dir}/16_filter_non_virus_blastn_nodes/{{sample}}.txt")
    log:
        f"{log_dir}/16_filter_non_virus_blastn_nodes/{{sample}}.log"
    threads: 1
    benchmark:
        f"{log_dir}/16_filter_non_virus_blastn_nodes/{{sample}}.benchmark.txt"
    script:
        "scripts/filter_non_virus_megablast_nodes.py"

rule merge_annotated_blast_results:
    """
    Create file with merged megablast and blastn results.
    """
    input:
        megablast = f"{local_output_dir}/12_filter_non_virus_megablast_nodes/{{sample}}.txt",
        blastn = f"{local_output_dir}/16_filter_non_virus_blastn_nodes/{{sample}}.txt"
    output:
        merged = f"{local_output_dir}/17_merge_annotated_blast_results/{{sample}}.txt"
    threads: 1
    log:
        f"{log_dir}/17_merge_annotated_blast_results/{{sample}}.log"
    benchmark:
        f"{log_dir}/17_merge_annotated_blast_results/{{sample}}.benchmark.txt"
    shell:
        """
        if [ -s {input.megablast} ] || [ -s {input.blastn} ]; then
            cat {input.megablast} {input.blastn} > {output.merged} 2> {log}
        else
            touch {output.merged}
            echo "Both input files are empty. Created an empty output file." > {log}
        fi
        """

rule extract_unclassified_contigs:
    """
    Extract unclassified contigs that do not have BLAST hits.
    These could be highly divergent viruses, but are most often junk.
    """
    input:
        contigs = f"{local_output_dir}/09_extract_human_virus_contigs/{{sample}}.fa",
        megablast_results = f"{local_output_dir}/10_megablast/{{sample}}.txt",
        blastn_results = f"{local_output_dir}/14_blastn_classify/{{sample}}.txt"
    output:
        unclassified = touch(f"{local_output_dir}/18_extract_unclassified_contigs/{{sample}}.unclassified.fa")
    threads: 1
    resources:
        mem_mb = 4000
    log:
        f"{log_dir}/18_extract_unclassified_contigs/{{sample}}.log"
    benchmark:
        f"{log_dir}/18_extract_unclassified_contigs/{{sample}}.benchmark.txt"
    script:
        "scripts/extract_unclassified_contigs.py"

## Load results into LabKey ##
# Import the BLAST output into a LabKey table for exploration
# Import contig FASTA sequences into a LabKey table for easy access

rule labkey_upload_blast:
    """
    Uploads BLAST results to LabKey or saves to CSV if LabKey API is missing.
    """
    input:
        blast_results = f"{local_output_dir}/17_merge_annotated_blast_results/{{sample}}.txt",
        mapped_reads = f"{local_output_dir}/20_map_reads_to_contigs/{{sample}}_mapped_counts.txt",
        fastq_file = f"{local_output_dir}/01_prepare_input/{{sample}}.fq.gz"
    output:
        token = touch(f"{local_output_dir}/19_labkey/{{sample}}.blast.tkn"),
        csv_file = f"{config['global']['out_dir']}/{{sample}}.csv"  # Ensures the CSV is always an output
    threads: 1
    resources:
        mem_mb = 4000
    params:
        experiment = config["global"]["experiment"],
        blast_db_name = config["global"]["blast_db_name"],
        snakemake_run_id = snakemake_run_id,
        labkey_server = config["global"].get("labkey_server", ""),
        project_name = config["global"].get("labkey_project_name", ""),
        api_key = config["global"].get("labkey_api_key", ""),
        stat_db_version = config["global"]["stat_dbss"],
        out_dir = config["global"]["out_dir"]
    log:
        f"{log_dir}/19_labkey/{{sample}}.blast.log"
    benchmark:
        f"{log_dir}/19_labkey/{{sample}}.blast.benchmark.txt"
    script:
        "scripts/labkey_upload.py"

rule labkey_upload_fasta:
    """
    Uploads FASTA results to LabKey data explorer
    """
    input:
        fasta = f"{local_output_dir}/09_extract_human_virus_contigs/{{sample}}.fa"
    output:
        token = touch(f"{local_output_dir}/19_labkey/{{sample}}.fasta.tkn")
    threads: 1
    log:
        f"{log_dir}/19_labkey{{sample}}.fasta.log"
    benchmark:
        f"{log_dir}/19_labkey/{{sample}}.fasta.txt"
    params:
        experiment = config["global"]["experiment"],
        labkey_server = config["global"].get("labkey_server", ""),
        project_name = config["global"].get("labkey_project_name", ""),        
        api_key = config["global"].get("labkey_api_key", ""),
        snakemake_run_id = snakemake_run_id,
        singleline_fasta_path = f"{local_output_dir}/19_labkey/{{sample}}.fasta"
    script:
        "scripts/labkey_upload_fasta.py"

rule map_reads_to_contigs:
    """
    Maps potential human virus reads to identified SPAdes contigs.
    It's important for validating the assembled viral sequences and assessing their coverage.
    """
    input:
        contigs = f"{local_output_dir}/09_extract_human_virus_contigs/{{sample}}.fa",
        reads = f"{local_output_dir}/02_extract_human_virus_reads/{{sample}}.fq.gz"
    output:
        bam = f"{local_output_dir}/20_map_reads_to_contigs/{{sample}}.bam",
        bai = f"{local_output_dir}/20_map_reads_to_contigs/{{sample}}.bam.bai"
    threads: 4
    log:
        f"{log_dir}/20_map_reads_to_contigs/{{sample}}.map.log"
    benchmark:
        f"{log_dir}/20_map_reads_to_contigs/{{sample}}.map.benchmark.txt"
    params:
        mapper = lambda wildcards, input: "map-ont" if get_input_type(wildcards, config) in ['ont', 'sra'] else "sr"
    shell:
        """
        (minimap2 -ax {params.mapper} -t {threads} {input.contigs} {input.reads} | \
        samtools view -b -F 4 | \
        samtools sort -@ {threads} -o {output.bam} && \
        samtools index {output.bam}) 2>&1 | tee {log}
        """

rule count_mapped_reads:
    """
    Counts the number of reads mapped to each contig in the BAM file.
    Outputs a two_column file with contig name and the number of mapped reads.
    samtools flag 2304 to retain only primary alignments in counting
    """
    input:
        bam = f"{local_output_dir}/20_map_reads_to_contigs/{{sample}}.bam"
    output:
        filtered_bam=temp(f"{local_output_dir}/20_map_reads_to_contigs/{{sample}}.filtered.bam"),
        filtered_bam_index=temp(f"{local_output_dir}/20_map_reads_to_contigs/{{sample}}.filtered.bam.bai"),
        counts = f"{local_output_dir}/20_map_reads_to_contigs/{{sample}}_mapped_counts.txt"
    log:
        f"{log_dir}/20_map_reads_to_contigs/{{sample}}.count.log"
    benchmark:
        f"{log_dir}/20_map_reads_to_contigs/{{sample}}.count.benchmark.txt"
    shell:
        """
        samtools view -F 2304 -b {input.bam} > {output.filtered_bam}
        samtools index {output.filtered_bam}
        samtools idxstats {output.filtered_bam} | awk '{{print $1, $3}}' > {output.counts} 2>&1 | tee {log}
        """

rule create_tarball:
    input:
        virus_contigs = f"{local_output_dir}/09_extract_human_virus_contigs/{{sample}}.fa",
        unclassified_contigs = f"{local_output_dir}/18_extract_unclassified_contigs/{{sample}}.unclassified.fa",
        mapped_reads = f"{local_output_dir}/20_map_reads_to_contigs/{{sample}}.bam",
        mapped_reads_index = f"{local_output_dir}/20_map_reads_to_contigs/{{sample}}.bam.bai"
    output:
        tarball = f"{local_output_dir}/21_create_tarball/{{sample}}.tar.zst"
    log:
        f"{log_dir}/21_create_tarball/{{sample}}.log"
    shell:
        """
        (
            # Initialize an empty list to hold the valid files
            valid_files=""
            
            # Check each file for existence and non-zero size
            for file in {input.virus_contigs} {input.unclassified_contigs} {input.mapped_reads} {input.mapped_reads_index}; do
                if [ -s "$file" ]; then
                    valid_files="$valid_files $file"
                else
                    echo "Skipping missing or empty file: $file" >> {log}
                fi
            done
            
            # Only create the tarball if there are valid files
            if [ -n "$valid_files" ]; then
                tar -I zstd -cvf {output.tarball} $valid_files >> {log} 2>&1
            else
                echo "No valid files to include in the tarball." >> {log}
            fi
        ) >> {log} 2>&1
        """
        
rule upload_files_to_labkey:
    input:
        tarball = rules.create_tarball.output.tarball,
        config_file = "config/config.yaml",
    output:
        done = f"{local_output_dir}/19_labkey/{{sample}}.upload.done"
    params:
        labkey_server = config["global"].get("labkey_server", ""),
        project_name = config["global"].get("labkey_project_name", ""),   
        webdav_url = config["global"].get("webdav_url", ""),
        username = config["global"].get("labkey_username", ""),
        password = config["global"].get("labkey_password", ""),
        experiment = config["global"]["experiment"],
        snakemake_run_id = snakemake_run_id,
        final_output_path = config["global"]["out_dir"]
    log:
        f"{log_dir}/19_labkey/{{sample}}.files.log"
    script:
        "scripts/upload_files_to_labkey.py"

## 2024-11-29 - add microbial non-viral targets ##
# find reads that map exactly to FDA-ARGOS targets

rule all_microbial:
    input:
        # Decompress references
        expand(f"{reference_decompressed_dir}/{{reference}}.fasta.gz", reference=reference_list),
        # Filtered BAM files
        expand(
            f"{local_output_dir}/22_map_reads_to_references/{{sample}}/{{reference}}.filtered.bam",
            sample=[s['name'] for s in config['samples']],
            reference=reference_list
        ),
        # Mapped counts
        expand(
            f"{local_output_dir}/22_map_reads_to_references/{{sample}}/{{reference}}_mapped_counts.txt",
            sample=[s['name'] for s in config['samples']],
            reference=reference_list
        ),
        # Total reads counts
        expand(
            f"{local_output_dir}/22_map_reads_to_references/{{sample}}/total_reads.txt",
            sample=[s['name'] for s in config['samples']]
        ),
        # Aggregate CSV
        f"{local_output_dir}/22_map_reads_to_references/aggregate_mapping.csv"

rule temp_microbial:
    input:
        # Decompress references
        expand(f"{reference_decompressed_dir}/{{reference}}.fasta.gz", reference=reference_list),
        # Baited sequences
        expand(
            f"{local_output_dir}/26_hq_argos_contigs/{{sample}}/{{sample}}.bam",
            sample=[s['name'] for s in config['samples']],
            reference=reference_list
        ),
        expand(
            f"{local_output_dir}/25_mapped_reads/{{sample}}/{{sample}}.mapped_reads.txt",
            sample=[s['name'] for s in config['samples']],
            reference=reference_list
        ),
        expand(
            f"{local_output_dir}/26_hq_argos_contigs/{{sample}}/{{sample}}.fa.gz",
            sample=[s['name'] for s in config['samples']],
            reference=reference_list
        ),
        f"{local_output_dir}/27_argos_aggregated/aggregated_results.csv"

rule decompress_argos_reference_archive:
    """
    Decompress the FDA-ARGOS reference archive and ensure reference files are directly within the target directory.
    """
    input:
        archive = reference_archive
    output:
        references = expand(f"{reference_decompressed_dir}/{{reference}}.fasta.gz", reference=reference_list)
    shell:
        """
        mkdir -p {reference_decompressed_dir}
        tar -I zstd -xvf {input.archive} -C {reference_decompressed_dir}
        
        # Move inner directory contents up if necessary
        # Needed depending on how .tar.zst archive is created
        inner_dir=$(find {reference_decompressed_dir} -mindepth 1 -maxdepth 1 -type d)
        if [ -d "$inner_dir" ]; then
            mv "$inner_dir"/* {reference_decompressed_dir}/
            rmdir "$inner_dir"
        fi
        """

rule concatenate_argos_fasta_for_baiting:
    """
    Concatenate all the ARGOS sequences into a single compressed FASTA file.
    The FASTA file is a mapping target for baiting - finding all possible exact matches first
    So that exhaustive searching is far more efficient.
    """
    input:
        references = expand(f"{reference_decompressed_dir}/{{reference}}.fasta.gz", reference=reference_list)
    output:
        "resources/argos.fa.gz"
    params:
        dir = reference_decompressed_dir
    shell:
        """
        find {params.dir} -name '*.fasta.gz' -exec cat {{}} + > {output}
        """

rule argos_mapping:
    """
    Map reads to concatenated ARGOS sequences.
    For Illumina reads, use bbmap to retain only perfect paired read matches
    Use trimreaddescriptions=t to match ONT BAM format
    For ONT reads, use minimap2 and then reformat.sh to retain subfilter=0/clipfilter=0 mapped reads
    Then sort and index BAM
    """
    input:
        reads = f"{local_output_dir}/01_prepare_input/{{sample}}.fq.gz",
        reference = "resources/argos.fa.gz"
    output:
        bam = f"{local_output_dir}/23_argos_mapping/{{sample}}/{{sample}}.argos.bam",
        bai = f"{local_output_dir}/23_argos_mapping/{{sample}}/{{sample}}.argos.bam.bai"
    threads: 4
    params:
        mapper = lambda wildcards: "sr" if get_input_type(wildcards, config) not in ['ont', 'sra'] else "map-ont"
    log:
        f"{log_dir}/23_argos_mapping/{{sample}}/argos_baited.log"
    shell:
       """
        if [ "{params.mapper}" = "sr" ]; then
            # Mapping with BBMap for short reads
            {{
                bbmap.sh \
                    ref={input.reference} \
                    in={input.reads} \
                    outm=stdout.bam \
                    perfectmode=t \
                    threads={threads} \
                    mappedonly=t \
                    pairedonly=t \
                    ow=t \
                    nodisk=t \
                    ambig=all \
                    trimreaddescriptions=t \
                    -Xmx64g \
                | samtools sort -o {output.bam} - \
                    && samtools index {output.bam}
            }} > {log} 2>&1
        
        else
            # Mapping with Minimap2 for long reads
            {{ 
                minimap2 --eqx -ax {params.mapper} -t {threads} {input.reference} {input.reads} \
                | reformat.sh \
                    in=stdin.sam \
                    ref={input.reference} \
                    out=stdout.bam \
                    subfilter=0 \
                    clipfilter=0 \
                    mappedonly=t \
                    -Xmx64g \
                | samtools sort -o {output.bam} - \
                    && samtools index {output.bam}
            }} > {log} 2>&1
        fi
        """

rule argos_genome_coverage:
    """
    Compute genome coverage for each contig.
    Use samtools depth to determine per-base coverage,
    then awk to compute the number of bases with coverage per contig.
    """
    input:
        bam = f"{local_output_dir}/23_argos_mapping/{{sample}}/{{sample}}.argos.bam"
    output:
        gc = f"{local_output_dir}/24_genome_coverage/{{sample}}/{{sample}}.gc.txt"
    threads: 1
    log:
        f"{log_dir}/24_genome_coverage/{{sample}}/{{sample}}.gc.log"
    benchmark:
        f"{log_dir}/24_genome_coverage/{{sample}}/{{sample}}.gc.benchmark"
    shell:
        """
        {{
            samtools depth {input.bam} \
            | awk '{{cov[$1] += ($3 > 0)}} END {{for (c in cov) print c, cov[c]}}' \
            > {output.gc}
        }} > {log} 2>&1
        """

rule argos_mapped_reads_per_contig:
    """
    Compute the number of mapped reads for each contig.
    Filters out contigs with zero mapped reads.
    Uses samtools idxstats to obtain per-contig read counts.
    """
    input:
        bam = f"{local_output_dir}/23_argos_mapping/{{sample}}/{{sample}}.argos.bam"
    output:
        mapped_reads = f"{local_output_dir}/25_mapped_reads/{{sample}}/{{sample}}.mapped_reads.txt"
    threads: 1
    log:
        f"{log_dir}/25_mapped_reads/{{sample}}/{{sample}}.mapped_reads.log"
    benchmark:
        f"{log_dir}/25_mapped_reads/{{sample}}/{{sample}}.mapped_reads.benchmark"
    shell:
        """
        samtools idxstats {input.bam} | \
        awk '$3 >= 1 {{print $1, $3}}' > {output.mapped_reads} 2> {log}
        """

rule hq_argos_contigs:
    """
    Extract contig names with genome coverage >= 500 bp and create a BED file.
    """
    input:
        coverage = f"{local_output_dir}/24_genome_coverage/{{sample}}/{{sample}}.gc.txt",
        idxstats = f"{local_output_dir}/23_argos_mapping/{{sample}}/{{sample}}.argos.bam"
    output:
        bed = f"{local_output_dir}/26_hq_argos_contigs/{{sample}}/{{sample}}.hq_argos_contigs.bed"
    log:
        f"{log_dir}/26_hq_argos_contigs/{{sample}}/{{sample}}.hq_argos_contigs.log"
    benchmark:
        f"{log_dir}/26_hq_argos_contigs/{{sample}}/{{sample}}.hq_argos_contigs.benchmark"
    shell:
        """
        # Extract contig lengths
        samtools idxstats {input.idxstats} > {wildcards.sample}_contig_lengths.txt
        
        # Join coverage file with contig lengths on contig name
        awk 'NR==FNR{{cov[$1]=$2; next}} $1 in cov && cov[$1]>=500 {{print $1"\t0\t"$2}}' {input.coverage} {wildcards.sample}_contig_lengths.txt > {output.bed}
        
        # Clean up
        rm {wildcards.sample}_contig_lengths.txt
        """

rule extract_hq_bam:
    """
    Create a new BAM file containing only contigs with genome coverage >= 500 bp.
    """
    input:
        bam = f"{local_output_dir}/23_argos_mapping/{{sample}}/{{sample}}.argos.bam",
        bed = f"{local_output_dir}/26_hq_argos_contigs/{{sample}}/{{sample}}.hq_argos_contigs.bed"
    output:
        filtered_bam = f"{local_output_dir}/26_hq_argos_contigs/{{sample}}/{{sample}}.bam"
    log:
        f"{log_dir}/26_hq_argos_contigs/{{sample}}/{{sample}}.bam.log"
    benchmark:
        f"{log_dir}/26_hq_argos_contigs/{{sample}}/{{sample}}.bam.benchmark"
    shell:
        """
        samtools view -b -L {input.bed} {input.bam} > {output.filtered_bam} 2> {log}
        """

rule extract_hq_fasta:
    """
    Extract reference sequences corresponding to contigs with genome coverage >= 500 bp.
    """
    input:
        bed = f"{local_output_dir}/26_hq_argos_contigs/{{sample}}/{{sample}}.hq_argos_contigs.bed",
        reference_fasta = "resources/argos.fa.gz"
    output:
        contig_names = temp(f"{local_output_dir}/26_hq_argos_contigs/{{sample}}/{{sample}}.contigs.txt"),
        filtered_fasta = f"{local_output_dir}/26_hq_argos_contigs/{{sample}}/{{sample}}.fa.gz"
    log:
        f"{log_dir}/26_hq_argos_contigs/{{sample}}/{{sample}}.fa.log"
    threads: 1
    shell:
        """
        # Extract contig names from the BED file
        awk '{{print $1}}' {input.bed} > {output.contig_names}
        
        # Use seqkit to extract the corresponding sequences from the reference FASTA
        seqkit grep -f {output.contig_names} {input.reference_fasta} | gzip > {output.filtered_fasta} 2> {log}
        """

rule count_total_reads:
    """
    Count the total number of reads in each sample's FASTQ file using seqkit.
    """
    input:
        reads = f"{local_output_dir}/01_prepare_input/{{sample}}.fq.gz"
    output:
        total_reads = f"{local_output_dir}/27_argos_aggregated/{{sample}}.total_reads.txt"
    log:
        f"{log_dir}/27_argos_aggregated/{{sample}}.total.log"
    shell:
        """
        seqkit stats -T {input.reads} | awk 'NR==2 {{print $4}}' > {output.total_reads} 2> {log}
        """

rule aggregate_results:
    """
    Aggregate results from multiple samples into a single CSV file.
    """
    input:
        gc_files=lambda wildcards: expand(
            f"{local_output_dir}/24_genome_coverage/{{sample}}/{{sample}}.gc.txt",
            sample=[s['name'] for s in config["samples"]]
        ),
        mapped_reads_files=lambda wildcards: expand(
            f"{local_output_dir}/25_mapped_reads/{{sample}}/{{sample}}.mapped_reads.txt",
            sample=[s['name'] for s in config["samples"]]
        ),
        bed_files=lambda wildcards: expand(
            f"{local_output_dir}/26_hq_argos_contigs/{{sample}}/{{sample}}.hq_argos_contigs.bed",
            sample=[s['name'] for s in config["samples"]]
        ),
        total_reads_files=lambda wildcards: expand(
            f"{local_output_dir}/27_argos_aggregated/{{sample}}.total_reads.txt",
            sample=[s['name'] for s in config["samples"]]
        ),
        filtered_fasta_files=lambda wildcards: expand(
            f"{local_output_dir}/26_hq_argos_contigs/{{sample}}/{{sample}}.fa.gz",
            sample=[s['name'] for s in config["samples"]]
        )
    output:
        aggregated_csv = f"{local_output_dir}/27_argos_aggregated/aggregated_results.csv"
    params:
        experiment = config["global"]["experiment"],
        argos_version = config["global"]["reference_archive"],
        snakemake_run_id = config["run_id"]
    run:
        import pandas as pd
        import gzip
        from Bio import SeqIO

        # Initialize an empty list to collect data
        aggregated_data = []

        # List of sample names
        samples = [s['name'] for s in config["samples"]]

        # Map sample names to input files
        sample_gc = dict(zip(samples, input.gc_files))
        sample_mapped_reads = dict(zip(samples, input.mapped_reads_files))
        sample_bed = dict(zip(samples, input.bed_files))
        sample_total_reads = dict(zip(samples, input.total_reads_files))
        sample_fasta = dict(zip(samples, input.filtered_fasta_files))

        for sample in samples:
            print(f"Processing sample: {sample}")

            # Get total_reads from total_reads file
            total_reads_file = sample_total_reads[sample]
            try:
                with open(total_reads_file, 'r') as f:
                    total_reads_str = f.read().strip()
                    if total_reads_str:
                        total_reads = int(float(total_reads_str))
                    else:
                        total_reads = 0
                        print(f"Warning: Total reads file {total_reads_file} is empty for sample {sample}. Setting to 0.")
            except (FileNotFoundError, ValueError):
                total_reads = 0
                print(f"Warning: Unable to read total_reads for sample {sample}. Setting to 0.")

            # Read genome coverage
            gc_file = sample_gc[sample]
            try:
                gc_df = pd.read_csv(
                    gc_file, sep=' ', header=None, names=['argos_id', 'genome_coverage']
                )
            except pd.errors.EmptyDataError:
                gc_df = pd.DataFrame(columns=['argos_id', 'genome_coverage'])
                print(f"Warning: Genome coverage file {gc_file} is empty for sample {sample}.")

            # Read mapped reads
            mapped_reads_file = sample_mapped_reads[sample]
            try:
                mapped_reads_df = pd.read_csv(
                    mapped_reads_file, sep=' ', header=None, names=['argos_id', 'mapped_reads']
                )
            except pd.errors.EmptyDataError:
                mapped_reads_df = pd.DataFrame(columns=['argos_id', 'mapped_reads'])
                print(f"Warning: Mapped reads file {mapped_reads_file} is empty for sample {sample}.")

            # Read BED file to get contig lengths
            bed_file = sample_bed[sample]
            try:
                bed_df = pd.read_csv(
                    bed_file, sep='\t', header=None, names=['argos_id', 'start', 'end']
                )
                bed_df['argos_contig_length'] = bed_df['end'] - bed_df['start']
            except pd.errors.EmptyDataError:
                bed_df = pd.DataFrame(columns=['argos_id', 'start', 'end', 'argos_contig_length'])
                print(f"Warning: BED file {bed_file} is empty for sample {sample}.")

            # Read filtered_fasta to get argos_contig mapping
            fasta_file = sample_fasta[sample]
            contig_to_desc = {}
            try:
                with gzip.open(fasta_file, 'rt') as handle:
                    for record in SeqIO.parse(handle, "fasta"):
                        header = record.description  # full header
                        # Split the header into argos_id and argos_contig
                        parts = header.split(None, 1)  # Split on first whitespace
                        if len(parts) == 2:
                            argos_id, argos_contig_desc = parts
                        elif len(parts) == 1:
                            argos_id = parts[0]
                            argos_contig_desc = parts[0]
                        else:
                            # Handle cases with no header
                            continue
                        contig_to_desc[argos_id] = argos_contig_desc
            except FileNotFoundError:
                print(f"Warning: FASTA file {fasta_file} not found for sample {sample}.")
            except gzip.BadGzipFile:
                print(f"Warning: FASTA file {fasta_file} is not a valid gzip file for sample {sample}.")

            # Assign 'argos_contig' based on 'argos_id'
            if not mapped_reads_df.empty:
                # Assign argos_contig using the mapping
                mapped_reads_df['argos_contig'] = mapped_reads_df['argos_id'].map(contig_to_desc)
            else:
                mapped_reads_df['argos_contig'] = pd.Series(dtype='object')

            # Merge dataframes on 'argos_id'
            df = pd.merge(gc_df, mapped_reads_df, on='argos_id', how='outer')
            df = pd.merge(df, bed_df[['argos_id', 'argos_contig_length']], on='argos_id', how='outer')

            # Fill missing values with zeros
            df['genome_coverage'] = df['genome_coverage'].fillna(0).astype(int)
            df['mapped_reads'] = df['mapped_reads'].fillna(0).astype(int)
            df['argos_contig_length'] = df['argos_contig_length'].fillna(0).astype(int)

            # Add additional columns
            df['sample_id'] = sample
            df['experiment'] = params.experiment
            df['argos_version'] = params.argos_version
            df['snakemake_run_id'] = params.snakemake_run_id
            df['total_reads'] = total_reads

            # Reorder columns
            df = df[[
                'experiment',
                'sample_id',
                'argos_contig',
                'argos_id',
                'genome_coverage',
                'mapped_reads',
                'total_reads',
                'argos_contig_length',
                'argos_version',
                'snakemake_run_id'
            ]]

            # Append to aggregated_data
            aggregated_data.append(df)

        if aggregated_data:
            # Concatenate all dataframes
            aggregated_df = pd.concat(aggregated_data, ignore_index=True)
        else:
            # Create an empty DataFrame with the required columns if no data is present
            aggregated_df = pd.DataFrame(columns=[
                'experiment',
                'sample_id',
                'argos_contig',
                'argos_id',
                'genome_coverage',
                'mapped_reads',
                'total_reads',
                'argos_contig_length',
                'argos_version',
                'snakemake_run_id'
            ])

        # Drop rows where 'argos_id' or 'argos_contig' is missing
        aggregated_df = aggregated_df.dropna(subset=['argos_id', 'argos_contig'])

        # Ensure 'argos_id' and 'argos_contig' are strings
        aggregated_df['argos_id'] = aggregated_df['argos_id'].astype(str)
        aggregated_df['argos_contig'] = aggregated_df['argos_contig'].astype(str)

        # Remove duplicate rows
        aggregated_df = aggregated_df.drop_duplicates()

        # Sort by sample_id, then by largest number of mapped reads
        aggregated_df = aggregated_df.sort_values(['sample_id', 'mapped_reads'], ascending=[True, False])

        # Write to CSV
        aggregated_df.to_csv(output.aggregated_csv, index=False)


## Cleanup ##
# Utility rule to run in isolation to remove existing output files.

rule clean:
    """
    Remove all generated files and directories.
    """
    params:
        local_output_dir = local_output_dir,
        dirs_to_remove = [
            "results",
            "fasterq.tmp*",
            "*.temp"
        ]
    script:
        "scripts/clean.py"